{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:37:46.459996Z",
     "iopub.status.busy": "2023-11-17T13:37:46.459436Z",
     "iopub.status.idle": "2023-11-17T13:37:49.757695Z",
     "shell.execute_reply": "2023-11-17T13:37:49.756700Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 16:03:27.646990: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-15 16:03:27.880651: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-15 16:03:27.881820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 16:03:30.040363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:37:49.762806Z",
     "iopub.status.busy": "2023-11-17T13:37:49.761841Z",
     "iopub.status.idle": "2023-11-17T13:38:15.833950Z",
     "shell.execute_reply": "2023-11-17T13:38:15.833244Z"
    },
    "id": "pOdqCMoQDRJL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis.castano.ortega/.local/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi Roy hope you are ok, Trans people are not g...</td>\n",
       "      <td>95e98db99c2</td>\n",
       "      <td>Hi Roy hope you are ok, Trans people are not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>But fuckin' hell what even is biology</td>\n",
       "      <td>a87b8708e63</td>\n",
       "      <td>But fuckin' hell what even is biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Whose the nice looking clergyman?</td>\n",
       "      <td>5b3cb03803f</td>\n",
       "      <td>Whose the nice looking clergyman?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AIDS ARE IN YOUR WAY, SIN HAS CONSEQUENCES AND...</td>\n",
       "      <td>5b0bad2347e</td>\n",
       "      <td>AIDS ARE IN YOUR WAY, SIN HAS CONSEQUENCES AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>to learn</td>\n",
       "      <td>332048bd188</td>\n",
       "      <td>to learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah...if we still alive at the time of the ne...</td>\n",
       "      <td>9141edf7189</td>\n",
       "      <td>Yeah...if we still alive at the time of the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8144</th>\n",
       "      <td>0</td>\n",
       "      <td>There is something fundamentally wrong with this</td>\n",
       "      <td>fde30735725</td>\n",
       "      <td>There is something fundamentally wrong with this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8145</th>\n",
       "      <td>1</td>\n",
       "      <td>This always confused me. If homosexuality is \"...</td>\n",
       "      <td>3d3456b6659</td>\n",
       "      <td>This always confused me. If homosexuality is \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146</th>\n",
       "      <td>1</td>\n",
       "      <td>disgusting</td>\n",
       "      <td>eace075fe4d</td>\n",
       "      <td>disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8147</th>\n",
       "      <td>0</td>\n",
       "      <td>Peter Sørensen Just a note on Peter's poor mat...</td>\n",
       "      <td>da703acd01e</td>\n",
       "      <td>Peter Sørensen Just a note on Peter's poor mat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8148 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text           id  \\\n",
       "0         0  Hi Roy hope you are ok, Trans people are not g...  95e98db99c2   \n",
       "1         0              But fuckin' hell what even is biology  a87b8708e63   \n",
       "2         0                  Whose the nice looking clergyman?  5b3cb03803f   \n",
       "3         1  AIDS ARE IN YOUR WAY, SIN HAS CONSEQUENCES AND...  5b0bad2347e   \n",
       "4         0                                           to learn  332048bd188   \n",
       "...     ...                                                ...          ...   \n",
       "8143      1  Yeah...if we still alive at the time of the ne...  9141edf7189   \n",
       "8144      0   There is something fundamentally wrong with this  fde30735725   \n",
       "8145      1  This always confused me. If homosexuality is \"...  3d3456b6659   \n",
       "8146      1                                         disgusting  eace075fe4d   \n",
       "8147      0  Peter Sørensen Just a note on Peter's poor mat...  da703acd01e   \n",
       "\n",
       "                                             clean_text  \n",
       "0     Hi Roy hope you are ok, Trans people are not g...  \n",
       "1                 But fuckin' hell what even is biology  \n",
       "2                     Whose the nice looking clergyman?  \n",
       "3     AIDS ARE IN YOUR WAY, SIN HAS CONSEQUENCES AND...  \n",
       "4                                              to learn  \n",
       "...                                                 ...  \n",
       "8143  Yeah...if we still alive at the time of the ne...  \n",
       "8144   There is something fundamentally wrong with this  \n",
       "8145  This always confused me. If homosexuality is \"...  \n",
       "8146                                         disgusting  \n",
       "8147  Peter Sørensen Just a note on Peter's poor mat...  \n",
       "\n",
       "[8148 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset desde train.csv\n",
    "data = pd.read_csv('train.csv')\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo de idioma inglés de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    text = re.sub(url_pattern, 'URL', text)\n",
    "    \n",
    "    # Convertir emojis a palabras descriptivas\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Reemplazar emojis desemojizados con guiones bajos por espacios\n",
    "    emoji_pattern = r'\\b(\\w+(?:_\\w+)*)\\b'\n",
    "    text = re.sub(emoji_pattern, lambda x: ' '.join(x.group(1).split('_')), text)\n",
    "\n",
    "    # Reemplazar dos emojis juntos unidos por :: con un espacio entre ellos\n",
    "    double_emoji_pattern = r'::'\n",
    "    text = re.sub(double_emoji_pattern, ' ', text)\n",
    "    \n",
    "    # Eliminar símbolos '#' de los hashtags\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    '''\n",
    "    # Tokenización y lematización\n",
    "    doc = nlp(text)\n",
    "    tokens_lemmatized = [token.lemma_ for token in doc] \n",
    "    \n",
    "    # Eliminación de stopwords y caracteres especiales\n",
    "    clean_tokens = [token.lower() for token in tokens_lemmatized\n",
    "                    if not nlp.vocab[token].is_stop \n",
    "                    and nlp.vocab[token].is_oov\n",
    "                    and not nlp.vocab[token].is_punct\n",
    "                    and token.isalpha \n",
    "                   ]\n",
    "    \n",
    "    # Unir los tokens limpios en una cadena de texto nuevamente\n",
    "    clean_text = \" \".join(clean_tokens)\n",
    "    '''\n",
    "    return text\n",
    "\n",
    "def correct_spelling(text):\n",
    "    corrected_text = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        # Verificar si la palabra está mal escrita\n",
    "        if spell.unknown([word]):\n",
    "            # Corregir la palabra mal escrita\n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word:\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)  # Conservar la palabra original si la corrección es None\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "\n",
    "# Aplicar el preprocesamiento al texto\n",
    "#data=data[0:50]\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "data.head()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en entrenamiento y validación\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((data['clean_text'].values, data['label'].values))\n",
    "val_ds = train_ds.take(int(len(data) * 0.1))\n",
    "train_ds = train_ds.skip(int(len(data) * 0.1))\n",
    "\n",
    "# Definir el tamaño del lote y otros parámetros\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Preparar los datasets para entrenamiento y validación\n",
    "train_ds = train_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:19.980260Z",
     "iopub.status.busy": "2023-11-17T13:38:19.980014Z",
     "iopub.status.idle": "2023-11-17T13:38:20.006091Z",
     "shell.execute_reply": "2023-11-17T13:38:20.005449Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: b'Yes, what they were protesting??'\n",
      "Label : 0 \n",
      "Text: b\"It all went downhill when those illegal German immigrants began stealing our land back in the 5/6th centuries, just after we'd got rid of the bloody Italians...\"\n",
      "Label : 1 \n",
      "Text: b'Yes they are a really scary bunch of men and are coming in fast! I read 300 a day somewhere! Where do they hang out I wonder?'\n",
      "Label : 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 16:03:40.075640: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f'Text: {text_batch.numpy()[i]}')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:20.009960Z",
     "iopub.status.busy": "2023-11-17T13:38:20.009728Z",
     "iopub.status.idle": "2023-11-17T13:38:20.020795Z",
     "shell.execute_reply": "2023-11-17T13:38:20.020181Z"
    },
    "id": "y8_ctG55-uTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' #'small_bert/bert_en_uncased_L-4_H-512_A-8'  \n",
    "#@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:20.023963Z",
     "iopub.status.busy": "2023-11-17T13:38:20.023725Z",
     "iopub.status.idle": "2023-11-17T13:38:22.213761Z",
     "shell.execute_reply": "2023-11-17T13:38:22.212893Z"
    },
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:22.218861Z",
     "iopub.status.busy": "2023-11-17T13:38:22.218299Z",
     "iopub.status.idle": "2023-11-17T13:38:22.356966Z",
     "shell.execute_reply": "2023-11-17T13:38:22.356241Z"
    },
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## Using the BERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:22.360610Z",
     "iopub.status.busy": "2023-11-17T13:38:22.360137Z",
     "iopub.status.idle": "2023-11-17T13:38:27.135450Z",
     "shell.execute_reply": "2023-11-17T13:38:27.134643Z"
    },
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:27.139434Z",
     "iopub.status.busy": "2023-11-17T13:38:27.139145Z",
     "iopub.status.idle": "2023-11-17T13:38:27.387289Z",
     "shell.execute_reply": "2023-11-17T13:38:27.386546Z"
    },
    "id": "_OoF9mebuSZc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n",
      "Pooled Outputs Shape:(1, 768)\n",
      "Pooled Outputs Values:[ 0.17883591 -0.23532812  0.9570173  -0.390676   -0.2085972  -0.99966097\n",
      "  0.18750452  0.5010725  -0.4395941  -0.20681766  0.47851375 -0.47998708]\n",
      "Sequence Outputs Shape:(1, 128, 768)\n",
      "Sequence Outputs Values:[[ 0.23163535  0.37881243  0.20612031 ... -0.16753745  0.31791216\n",
      "  -0.05594946]\n",
      " [ 0.71144736  0.0524028   0.40399322 ... -0.23849721  0.06221148\n",
      "  -0.11749866]\n",
      " [ 0.31278408 -0.20193718  0.31673148 ...  0.224002    0.68846685\n",
      "  -0.67951345]\n",
      " ...\n",
      " [ 0.1932865   0.37243617  0.5798474  ... -0.25043333 -0.15593159\n",
      "   0.27656394]\n",
      " [ 0.26625976  0.2377773   0.5434538  ... -0.14922899 -0.23860906\n",
      "   0.24579929]\n",
      " [ 0.11940996  0.5005051   0.3698152  ... -0.18295774 -0.0871153\n",
      "   0.33573136]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## Define your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:27.391470Z",
     "iopub.status.busy": "2023-11-17T13:38:27.390765Z",
     "iopub.status.idle": "2023-11-17T13:38:27.396231Z",
     "shell.execute_reply": "2023-11-17T13:38:27.395553Z"
    },
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:27.399559Z",
     "iopub.status.busy": "2023-11-17T13:38:27.399047Z",
     "iopub.status.idle": "2023-11-17T13:38:32.973453Z",
     "shell.execute_reply": "2023-11-17T13:38:32.972716Z"
    },
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.38646916]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:33.107628Z",
     "iopub.status.busy": "2023-11-17T13:38:33.107356Z",
     "iopub.status.idle": "2023-11-17T13:38:33.120259Z",
     "shell.execute_reply": "2023-11-17T13:38:33.119650Z"
    },
    "id": "OWPOZE-L3AgE"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:33.123893Z",
     "iopub.status.busy": "2023-11-17T13:38:33.123364Z",
     "iopub.status.idle": "2023-11-17T13:38:33.129022Z",
     "shell.execute_reply": "2023-11-17T13:38:33.128427Z"
    },
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### Loading the BERT model and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:33.132489Z",
     "iopub.status.busy": "2023-11-17T13:38:33.132011Z",
     "iopub.status.idle": "2023-11-17T13:38:33.142268Z",
     "shell.execute_reply": "2023-11-17T13:38:33.141695Z"
    },
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBuV5j2cS_b"
   },
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T13:38:33.145780Z",
     "iopub.status.busy": "2023-11-17T13:38:33.145541Z",
     "iopub.status.idle": "2023-11-17T14:36:04.101969Z",
     "shell.execute_reply": "2023-11-17T14:36:04.101171Z"
    },
    "id": "HtfDFAnN_Neu",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n",
      "Epoch 1/2\n",
      " 15/230 [>.............................] - ETA: 28:51 - loss: 0.6692 - binary_accuracy: 0.6542"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T14:36:04.106409Z",
     "iopub.status.busy": "2023-11-17T14:36:04.105813Z",
     "iopub.status.idle": "2023-11-17T14:39:44.968184Z",
     "shell.execute_reply": "2023-11-17T14:39:44.967418Z"
    },
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(val_ds)\n",
    "predictions = classifier_model.predict(val_ds)\n",
    "'''\n",
    "modelo small L-4 H-512 A-8 \n",
    "no prepro 5 epochs         Loss: 1.0107866525650024 Accuracy: 0.7642725706100464\n",
    "\n",
    "prepro 5 epochs 8 batch    Loss: 0.9731619358062744 Accuracy: 0.7427870035171509\n",
    "\n",
    "3 epochs 16 batch lr 5e-5  Loss: 0.5888224244117737 Accuracy: 0.7519950866699219\n",
    "\n",
    "10 epochs 16 batch lr 1e-5 Loss: 0.6003473997116089 Accuracy: 0.7384898662567139\n",
    "\n",
    "15 epochs 16 batch lr 1e-5 Loss: 0.765673041343689  Accuracy: 0.7403314709663391\n",
    "\n",
    "10 epochs 16 batch lr 2e-5 Loss: 1.0079598426818848 Accuracy: 0.7446286082267761\n",
    "\n",
    "\n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.5700874924659729 Accuracy: 0.7446286082267761 \n",
    "\n",
    "10 epochs 32 batch lr 3e-5 Loss: 0.8490052223205566 Accuracy: 0.7409453392028809\n",
    "\n",
    "modelo small L-8 H-512 A-8 \n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.602721095085144  Accuracy: 0.7569060921669006\n",
    "no pre\n",
    "5 epochs 32 batch lr 2e-5  Loss: 0.5598483085632324 Accuracy: 0.7685696482658386 ////\\\\\\\n",
    "\n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.6556186676025391 Accuracy: 0.7685696482658386\n",
    "\n",
    "modelo small L-10 H-512 A-8 \n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.6236564517021179 Accuracy: 0.7599754333496094\n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.5313460230827332 Accuracy: 0.7776412963867188\n",
    "\n",
    "modelo small L-12 H-512 A-8 \n",
    "5 epochs 32 batch lr 3e-5  Loss: 0.7173242568969727 Accuracy: 0.7532228231430054\n",
    "6 Loss: 0.6913221478462219 Accuracy: 0.7605893015861511\n",
    "'''\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertir las probabilidades en etiquetas binarias (0 o 1)\n",
    "binary_predictions = (predictions > 0.0).astype(int)\n",
    "# Obtener las etiquetas verdaderas de los datos de validación\n",
    "true_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "\n",
    "# Mostrar la matriz de confusión como un mapa de calor\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(true_labels, binary_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T14:39:44.972229Z",
     "iopub.status.busy": "2023-11-17T14:39:44.971556Z",
     "iopub.status.idle": "2023-11-17T14:39:45.427481Z",
     "shell.execute_reply": "2023-11-17T14:39:45.426772Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos test\n",
    "test_data = pd.read_csv(\"test_nolabel.csv\")\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['clean_text'] = test_data['text'].apply(preprocess_text)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicción\n",
    "pred_new = classifier_model.predict(test_data[\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar umbral para convertir las predicciones en etiquetas binarias\n",
    "binary_predictions = (pred_new > 0.0).astype(int)\n",
    "\n",
    "# Añadir las predicciones binarias al DataFrame\n",
    "test_data['predicted_label'] = binary_predictions\n",
    "pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({'id': test_data[\"id\"], 'label': test_data['predicted_label']})\n",
    "\n",
    "predictions_df.to_csv(\"bert_predictions.csv\", index=False, header=True)\n",
    "predictions_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
